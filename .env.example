# Study Agent - Environment template
# Copy this file to backend/.env and fill in the secrets before running the server.

# -------------------------
# Cloud LLM configuration
# -------------------------
# Use either GOOGLE_API_KEY (preferred) or OPENAI_API_KEY. If neither is set,
# you may set USE_OLLAMA=true and configure Ollama locally.

# Google Gemini (Generative AI)
GOOGLE_API_KEY=your_google_api_key_here

# OpenAI (fallback)
OPENAI_API_KEY=your_openai_api_key_here

# -------------------------
# Local Ollama settings (optional)
# -------------------------
# Set USE_OLLAMA=true to use an Ollama server running locally (ollama serve)
USE_OLLAMA=false
OLLAMA_MODEL=mistral
OLLAMA_BASE_URL=http://localhost:11434

# -------------------------
# FAISS / storage
# -------------------------
# Local path where FAISS index and outputs are stored (relative to backend/)
FAISS_INDEX_PATH=./outputs/faiss_index

# Default LLM model name for OpenAI fallback (if using OpenAI)
LLM_MODEL=gpt-4o-mini
# Google Gemini API Configuration
# Get your API key from: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=your_google_api_key_here

# Optional: OpenAI API key (used if Google key not set)
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Use Ollama for local inference (set to true to enable)
USE_OLLAMA=false
OLLAMA_MODEL=mistral
OLLAMA_BASE_URL=http://localhost:11434

# Local FAISS path (relative to backend folder)
FAISS_INDEX_PATH=./outputs/faiss_index

# Default LLM model name for OpenAI fallback
LLM_MODEL=gpt-4o-mini
